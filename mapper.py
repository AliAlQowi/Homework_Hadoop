{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCq+RwzwLyfsi8Fn2pF+Rv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAlQowi/Homework_Hadoop/blob/main/mapper.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeyfiFchMykb",
        "outputId": "0ce675da-a00b-4625-b644-a7efe235d902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n",
            "openjdk version \"1.8.0_422\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_422-8u422-b05-1~22.04-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.422-b05, mixed mode)\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n",
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "#Port 22\n",
            "#GatewayPorts no\n",
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:rDoZCVWLYJsjMbkgZbFP2/b1Q7nazA3NwCyUi9twD2M root@f130036ab2aa\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|oo*. ..          |\n",
            "|+= =.. .   .     |\n",
            "|+.=.o .   o      |\n",
            "|...+ o . o + .   |\n",
            "|   .o.o S E *    |\n",
            "|    o. o B B =   |\n",
            "|     o. o . * o  |\n",
            "|    o.     = +   |\n",
            "|    ..    . + .  |\n",
            "+----[SHA256]-----+\n",
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDYmsH/EwIQ1n8f0xNSvoLwscL9aEj87RrXp7lvIR/YrmQeAk54s0y8oSDqNG9k\n",
            "jwMkGW7som5G3XG77iADGumQUnRXqaAX7BQZIy1hIVGnX7r0ukUUd0pIRcT6NW8qj483eONyywsaCC51U7shT7WLAr4VdKMb+PZx\n",
            "8Mls433xnHi83u6RYRSur5ByZVArYgFrRHwaH6cjSqvaJzFmpQLCEmXvdFDOEAxHIz54BFBocEkYPAhZNDFFq3X6lbKdRhGGux2R\n",
            "PfX2STBWw0paTvytQGK64IB87e5IzJ/4ltMVMYx/bKC3vC0wIG2wVjV5HgLoqtp1HcLK/FyieMnIhQ6G005lTogZOMM3pbbkhxaI\n",
            "OYusPHBWOXNQd+cHK1m9Jsf05kTsOFXhHJlFDVfcoaPVopLRT2hXlFJcLPMe34ZYH+2PIIbIzwUcv1naXzNfUNep5R+ah/KOSRqH\n",
            "FGUU3HNT4EhO/ghsweBtaN15INvOmrymaDgsg8rpqAlztsYJL9U= root@f130036ab2aa\n",
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
            " 16:20:15 up 5 min,  0 users,  load average: 0.87, 0.73, 0.37\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps\n",
        "!java -version\n",
        "\n",
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "!apt-get install openssh-server -qq > /dev/null\n",
        "!service ssh start\n",
        "\n",
        "!grep Port /etc/ssh/sshd_config\n",
        "\n",
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa <<< y\n",
        "\n",
        "# See id_rsa.pub content\n",
        "!more /root/.ssh/id_rsa.pub\n",
        "\n",
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime\n",
        "\n",
        "\n",
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "\n",
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively\n",
        "\n",
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "import os\n",
        "#Creating environment variables\n",
        "#Creating Hadoop home variable\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += f'{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"JRE_HOME\"]}/bin:{os.environ[\"HADOOP_HOME\"]}/sbin'\n",
        "\n",
        "#Dowloading text example to use as input\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format\n",
        "\n",
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\"\n",
        "\n",
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "\n",
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh\n",
        "\n",
        "#Listing the running deamons\n",
        "!jps\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnEgJXVSUVeQ",
        "outputId": "5138df9e-2f23-415f-f323-f2fa050841ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2024-08-19 16:22:52,887 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = f130036ab2aa/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_422\n",
            "************************************************************/\n",
            "2024-08-19 16:22:52,935 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-08-19 16:22:53,070 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-6e078060-8198-4b57-9a7c-4b25b9a95473\n",
            "2024-08-19 16:22:53,830 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-08-19 16:22:53,878 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-08-19 16:22:53,879 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-08-19 16:22:53,880 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-08-19 16:22:53,889 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2024-08-19 16:22:53,889 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2024-08-19 16:22:53,889 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2024-08-19 16:22:53,890 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-08-19 16:22:53,953 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-08-19 16:22:53,964 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2024-08-19 16:22:53,965 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-08-19 16:22:53,970 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-08-19 16:22:53,970 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Aug 19 16:22:53\n",
            "2024-08-19 16:22:53,972 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-08-19 16:22:53,973 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:22:53,974 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-08-19 16:22:53,974 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-08-19 16:22:54,007 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-08-19 16:22:54,007 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-08-19 16:22:54,014 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2024-08-19 16:22:54,014 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-08-19 16:22:54,014 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-08-19 16:22:54,015 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-08-19 16:22:54,058 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-08-19 16:22:54,058 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:22:54,058 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:22:54,058 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:22:54,073 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-08-19 16:22:54,074 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:22:54,074 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-08-19 16:22:54,074 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-08-19 16:22:54,077 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2024-08-19 16:22:54,077 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-08-19 16:22:54,077 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-08-19 16:22:54,077 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-08-19 16:22:54,083 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-08-19 16:22:54,085 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-08-19 16:22:54,090 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-08-19 16:22:54,090 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:22:54,090 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-08-19 16:22:54,090 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-08-19 16:22:54,100 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-08-19 16:22:54,100 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-08-19 16:22:54,100 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-08-19 16:22:54,104 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-08-19 16:22:54,104 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-08-19 16:22:54,106 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-08-19 16:22:54,106 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:22:54,107 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-08-19 16:22:54,107 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-08-19 16:22:54,153 INFO namenode.FSImage: Allocated new BlockPoolId: BP-932628457-172.28.0.12-1724084574135\n",
            "2024-08-19 16:22:54,186 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-08-19 16:22:54,231 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-08-19 16:22:54,342 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\n",
            "2024-08-19 16:22:54,361 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-08-19 16:22:54,409 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-08-19 16:22:54,410 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-08-19 16:22:54,415 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-08-19 16:22:54,415 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at f130036ab2aa/172.28.0.12\n",
            "************************************************************/\n",
            "Starting namenodes on [f130036ab2aa]\n",
            "f130036ab2aa: Warning: Permanently added 'f130036ab2aa' (ED25519) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [f130036ab2aa]\n",
            "nohup: ignoring input and appending output to 'nohup.out'\n",
            "3241 NodeManager\n",
            "3130 ResourceManager\n",
            "3357 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQNKHrgEUbgS",
        "outputId": "cad80eea-6a09-4ddd-e9eb-eda773d5413b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report: FileSystem file:/// is not an HDFS file system. The fs class is: org.apache.hadoop.fs.LocalFileSystem\n",
            "Usage: hdfs dfsadmin [-report] [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True: pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ogQH-71cUphx",
        "outputId": "725484b6-c446-4418-dded-e3bb74accd45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b16dc615ea65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "\n",
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count\n",
        "\n",
        "#Exploring Hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count\n",
        "\n",
        "# Run MapReduce Example using JAVA\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/pembukaan_uud1945.txt /word_count/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4biLWfSjUsMI",
        "outputId": "d8084f9f-28e9-4126-ad6b-bacd09b022c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root       1423 2024-08-19 16:24 /word_count/pembukaan_uud1945.txt\n",
            "2024-08-19 16:24:44,902 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-19 16:24:45,082 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-19 16:24:45,082 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-19 16:24:45,399 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-08-19 16:24:45,477 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-19 16:24:45,891 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1068782089_0001\n",
            "2024-08-19 16:24:45,892 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-19 16:24:46,301 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-19 16:24:46,303 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-19 16:24:46,304 INFO mapreduce.Job: Running job: job_local1068782089_0001\n",
            "2024-08-19 16:24:46,331 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:24:46,331 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:24:46,339 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-08-19 16:24:46,445 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-19 16:24:46,448 INFO mapred.LocalJobRunner: Starting task: attempt_local1068782089_0001_m_000000_0\n",
            "2024-08-19 16:24:46,519 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:24:46,522 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:24:46,601 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:24:46,609 INFO mapred.MapTask: Processing split: file:/word_count/pembukaan_uud1945.txt:0+1423\n",
            "2024-08-19 16:24:46,876 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-19 16:24:46,876 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-19 16:24:46,876 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-19 16:24:46,876 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-19 16:24:46,876 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-19 16:24:46,883 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-19 16:24:46,917 INFO mapred.LocalJobRunner: \n",
            "2024-08-19 16:24:46,917 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-19 16:24:46,917 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-19 16:24:46,917 INFO mapred.MapTask: bufstart = 0; bufend = 2125; bufvoid = 104857600\n",
            "2024-08-19 16:24:46,917 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213688(104854752); length = 709/6553600\n",
            "2024-08-19 16:24:46,981 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-19 16:24:47,024 INFO mapred.Task: Task:attempt_local1068782089_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:24:47,038 INFO mapred.LocalJobRunner: map\n",
            "2024-08-19 16:24:47,038 INFO mapred.Task: Task 'attempt_local1068782089_0001_m_000000_0' done.\n",
            "2024-08-19 16:24:47,055 INFO mapred.Task: Final Counters for attempt_local1068782089_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318072\n",
            "\t\tFILE: Number of bytes written=865158\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=2125\n",
            "\t\tMap output materialized bytes=1764\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=178\n",
            "\t\tCombine output records=120\n",
            "\t\tSpilled Records=120\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "2024-08-19 16:24:47,062 INFO mapred.LocalJobRunner: Finishing task: attempt_local1068782089_0001_m_000000_0\n",
            "2024-08-19 16:24:47,063 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-19 16:24:47,068 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-19 16:24:47,068 INFO mapred.LocalJobRunner: Starting task: attempt_local1068782089_0001_r_000000_0\n",
            "2024-08-19 16:24:47,091 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:24:47,091 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:24:47,092 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:24:47,096 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6a6366df\n",
            "2024-08-19 16:24:47,099 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:24:47,138 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-19 16:24:47,149 INFO reduce.EventFetcher: attempt_local1068782089_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-19 16:24:47,242 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1068782089_0001_m_000000_0 decomp: 1760 len: 1764 to MEMORY\n",
            "2024-08-19 16:24:47,254 INFO reduce.InMemoryMapOutput: Read 1760 bytes from map-output for attempt_local1068782089_0001_m_000000_0\n",
            "2024-08-19 16:24:47,256 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1760, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1760\n",
            "2024-08-19 16:24:47,260 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-19 16:24:47,263 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:24:47,269 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-19 16:24:47,282 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:24:47,283 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1752 bytes\n",
            "2024-08-19 16:24:47,287 INFO reduce.MergeManagerImpl: Merged 1 segments, 1760 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-19 16:24:47,289 INFO reduce.MergeManagerImpl: Merging 1 files, 1764 bytes from disk\n",
            "2024-08-19 16:24:47,290 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-19 16:24:47,290 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:24:47,292 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1752 bytes\n",
            "2024-08-19 16:24:47,293 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:24:47,300 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-19 16:24:47,308 INFO mapred.Task: Task:attempt_local1068782089_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:24:47,310 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:24:47,312 INFO mapred.Task: Task attempt_local1068782089_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-19 16:24:47,316 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1068782089_0001_r_000000_0' to file:/word_count/output\n",
            "2024-08-19 16:24:47,317 INFO mapreduce.Job: Job job_local1068782089_0001 running in uber mode : false\n",
            "2024-08-19 16:24:47,319 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-08-19 16:24:47,323 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-08-19 16:24:47,323 INFO mapred.Task: Task 'attempt_local1068782089_0001_r_000000_0' done.\n",
            "2024-08-19 16:24:47,324 INFO mapred.Task: Final Counters for attempt_local1068782089_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=321632\n",
            "\t\tFILE: Number of bytes written=868221\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=1764\n",
            "\t\tReduce input records=120\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=120\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:24:47,324 INFO mapred.LocalJobRunner: Finishing task: attempt_local1068782089_0001_r_000000_0\n",
            "2024-08-19 16:24:47,324 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-19 16:24:48,320 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-19 16:24:48,321 INFO mapreduce.Job: Job job_local1068782089_0001 completed successfully\n",
            "2024-08-19 16:24:48,331 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=639704\n",
            "\t\tFILE: Number of bytes written=1733379\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=2125\n",
            "\t\tMap output materialized bytes=1764\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=178\n",
            "\t\tCombine output records=120\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=1764\n",
            "\t\tReduce input records=120\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=240\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usyn6eK_VHBQ",
        "outputId": "6473590e-8354-48b7-b90e-a9b2383b8340"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-19 16:24 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1279 2024-08-19 16:24 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-wMrI5eVMz4",
        "outputId": "37b53c61-f155-43c8-e323-3ddb849f45ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "Negara\t4\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "Republik\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTSLzetZVTTo",
        "outputId": "42bbba67-3975-4c79-e12e-73fb258c374e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "4jayJiOFVVAH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python"
      ],
      "metadata": {
        "id": "8sPzg5GDVXxQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "WPV9VGMrVbuX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   with open('/content/pembukaan_uud1945.txt', 'r') as file:\n",
        "#         contents = file.read()\n",
        "#   print(contents)\n",
        "# except Exception as e:\n",
        "#     print(\"There is a Problem\", str(e))"
      ],
      "metadata": {
        "id": "lUP_I1HfVfYP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\n",
        "\n",
        "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\n",
        "\n",
        "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\n",
        "\n",
        "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia."
      ],
      "metadata": {
        "id": "7gWHqmwtVkh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the line into words, output data type list\n",
        "  words = line.split()\n",
        "\n",
        "  # we are looping over the words array and printing the word\n",
        "  # with the count of 1 to the STDOUT\n",
        "  for word in words:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    # what we output here will be the input for the\n",
        "    # Reduce step, i.e. the input for reducer.py\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LVWSe6yVloI",
        "outputId": "4a1b65f7-dac9-49d8-d889-b986d0bd6f65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/pembukaan_uud1945.txt', 'r') as file:\n",
        "#         line = file.read()\n",
        "line = \"\"\"\n",
        "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\n",
        "\n",
        "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\n",
        "\n",
        "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\n",
        "\n",
        "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia.\n",
        "\"\"\"\n",
        "# print(line)\n",
        "line = line.strip()\n",
        "words = line.split()\n",
        "\n",
        "for word in words:\n",
        "  data = '%s\\t%s' % (word, 1)\n",
        "\n",
        "  word, count = data.split('\\t', 1)\n",
        "  print(word, count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60dj7q0JVqKX",
        "outputId": "25bea374-a961-46f2-edcc-f8a11211a0fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bahwa 1\n",
            "sesungguhnya 1\n",
            "Kemerdekaan 1\n",
            "itu 1\n",
            "ialah 1\n",
            "hak 1\n",
            "segala 1\n",
            "bangsa 1\n",
            "dan 1\n",
            "oleh 1\n",
            "sebab 1\n",
            "itu, 1\n",
            "maka 1\n",
            "penjajahan 1\n",
            "di 1\n",
            "atas 1\n",
            "dunia 1\n",
            "harus 1\n",
            "dihapuskan, 1\n",
            "karena 1\n",
            "tidak 1\n",
            "sesuai 1\n",
            "dengan 1\n",
            "perikemanusiaan 1\n",
            "dan 1\n",
            "perikeadilan. 1\n",
            "Dan 1\n",
            "perjuangan 1\n",
            "pergerakan 1\n",
            "kemerdekaan 1\n",
            "Indonesia 1\n",
            "telah 1\n",
            "sampailah 1\n",
            "kepada 1\n",
            "saat 1\n",
            "yang 1\n",
            "berbahagia 1\n",
            "dengan 1\n",
            "selamat 1\n",
            "sentausa 1\n",
            "mengantarkan 1\n",
            "rakyat 1\n",
            "Indonesia 1\n",
            "ke 1\n",
            "depan 1\n",
            "pintu 1\n",
            "gerbang 1\n",
            "kemerdekaan 1\n",
            "Negara 1\n",
            "Indonesia, 1\n",
            "yang 1\n",
            "merdeka, 1\n",
            "bersatu, 1\n",
            "berdaulat, 1\n",
            "adil 1\n",
            "dan 1\n",
            "makmur. 1\n",
            "Atas 1\n",
            "berkat 1\n",
            "rakhmat 1\n",
            "Allah 1\n",
            "Yang 1\n",
            "Maha 1\n",
            "Kuasa 1\n",
            "dan 1\n",
            "dengan 1\n",
            "didorongkan 1\n",
            "oleh 1\n",
            "keinginan 1\n",
            "luhur, 1\n",
            "supaya 1\n",
            "berkehidupan 1\n",
            "kebangsaan 1\n",
            "yang 1\n",
            "bebas, 1\n",
            "maka 1\n",
            "rakyat 1\n",
            "Indonesia 1\n",
            "menyatakan 1\n",
            "dengan 1\n",
            "ini 1\n",
            "kemerdekaannya. 1\n",
            "Kemudian 1\n",
            "daripada 1\n",
            "itu 1\n",
            "untuk 1\n",
            "membentuk 1\n",
            "suatu 1\n",
            "Pemerintah 1\n",
            "Negara 1\n",
            "Indonesia 1\n",
            "yang 1\n",
            "melindungi 1\n",
            "segenap 1\n",
            "bangsa 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "seluruh 1\n",
            "tumpah 1\n",
            "darah 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "untuk 1\n",
            "memajukan 1\n",
            "kesejahteraan 1\n",
            "umum, 1\n",
            "mencerdaskan 1\n",
            "kehidupan 1\n",
            "bangsa, 1\n",
            "dan 1\n",
            "ikut 1\n",
            "melaksanakan 1\n",
            "ketertiban 1\n",
            "dunia 1\n",
            "yang 1\n",
            "berdasarkan 1\n",
            "kemerdekaan, 1\n",
            "perdamaian 1\n",
            "abadi 1\n",
            "dan 1\n",
            "keadilan 1\n",
            "sosial, 1\n",
            "maka 1\n",
            "disusunlah 1\n",
            "Kemerdekaan 1\n",
            "Kebangsaan 1\n",
            "Indonesia 1\n",
            "itu 1\n",
            "dalam 1\n",
            "suatu 1\n",
            "Undang-Undang 1\n",
            "Dasar 1\n",
            "Negara 1\n",
            "Indonesia, 1\n",
            "yang 1\n",
            "terbentuk 1\n",
            "dalam 1\n",
            "suatu 1\n",
            "susunan 1\n",
            "Negara 1\n",
            "Republik 1\n",
            "Indonesia 1\n",
            "yang 1\n",
            "berkedaulatan 1\n",
            "rakyat 1\n",
            "dengan 1\n",
            "berdasar 1\n",
            "kepada 1\n",
            "Ketuhanan 1\n",
            "Yang 1\n",
            "Maha 1\n",
            "Esa, 1\n",
            "Kemanusiaan 1\n",
            "yang 1\n",
            "adil 1\n",
            "dan 1\n",
            "beradab, 1\n",
            "Persatuan 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "Kerakyatan 1\n",
            "yang 1\n",
            "dipimpin 1\n",
            "oleh 1\n",
            "hikmat 1\n",
            "kebijaksanaan 1\n",
            "dalam 1\n",
            "Permusyawaratan/Perwakilan, 1\n",
            "serta 1\n",
            "dengan 1\n",
            "mewujudkan 1\n",
            "suatu 1\n",
            "Keadilan 1\n",
            "sosial 1\n",
            "bagi 1\n",
            "seluruh 1\n",
            "rakyat 1\n",
            "Indonesia. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKpVSnMBVwOJ",
        "outputId": "ab8d311b-80db-49a8-c40a-fb4b27a281a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat pembukaan_uud1945.txt | python mapper.py | sort -k1,1 | python reducer.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtItdkhtVxo3",
        "outputId": "91f3b017-30d6-403b-8046-a8a051a55af0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abadi\t1\n",
            "adil\t2\n",
            "Allah\t1\n",
            "atas\t1\n",
            "Atas\t1\n",
            "bagi\t1\n",
            "Bahwa\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "Dan\t1\n",
            "darah\t1\n",
            "daripada\t1\n",
            "Dasar\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n",
            "dipimpin\t1\n",
            "disusunlah\t1\n",
            "dunia\t2\n",
            "Esa,\t1\n",
            "gerbang\t1\n",
            "hak\t1\n",
            "harus\t1\n",
            "hikmat\t1\n",
            "ialah\t1\n",
            "ikut\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "ini\t1\n",
            "itu\t3\n",
            "itu,\t1\n",
            "karena\t1\n",
            "ke\t1\n",
            "keadilan\t1\n",
            "Keadilan\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "h9VGxYObV1vv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/pembukaan_uud1945.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpBYMQL8V3i4",
        "outputId": "a61d0523-7a3f-4054-af57-a1ec7ffacb1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-19 16:25:29,558 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-19 16:25:29,661 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-19 16:25:29,661 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-19 16:25:29,679 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:25:29,893 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-08-19 16:25:29,917 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-19 16:25:30,107 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local233258962_0001\n",
            "2024-08-19 16:25:30,107 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-19 16:25:30,296 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-19 16:25:30,298 INFO mapreduce.Job: Running job: job_local233258962_0001\n",
            "2024-08-19 16:25:30,306 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-19 16:25:30,308 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-08-19 16:25:30,313 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:25:30,313 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:25:30,359 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-19 16:25:30,365 INFO mapred.LocalJobRunner: Starting task: attempt_local233258962_0001_m_000000_0\n",
            "2024-08-19 16:25:30,399 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:25:30,405 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:25:30,431 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:25:30,442 INFO mapred.MapTask: Processing split: file:/word_count_with_python/pembukaan_uud1945.txt:0+1423\n",
            "2024-08-19 16:25:30,456 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-08-19 16:25:30,552 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-19 16:25:30,552 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-19 16:25:30,552 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-19 16:25:30,552 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-19 16:25:30,552 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-19 16:25:30,558 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-19 16:25:30,565 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper.py]\n",
            "2024-08-19 16:25:30,571 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-08-19 16:25:30,572 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-08-19 16:25:30,572 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-08-19 16:25:30,572 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-08-19 16:25:30,573 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-08-19 16:25:30,573 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-08-19 16:25:30,574 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-08-19 16:25:30,574 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-19 16:25:30,574 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-08-19 16:25:30,575 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-08-19 16:25:30,575 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-08-19 16:25:30,576 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-08-19 16:25:30,601 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:25:30,660 INFO streaming.PipeMapRed: Records R/W=7/1\n",
            "2024-08-19 16:25:30,665 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-19 16:25:30,667 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-19 16:25:30,672 INFO mapred.LocalJobRunner: \n",
            "2024-08-19 16:25:30,672 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-19 16:25:30,672 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-19 16:25:30,672 INFO mapred.MapTask: bufstart = 0; bufend = 1769; bufvoid = 104857600\n",
            "2024-08-19 16:25:30,672 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213688(104854752); length = 709/6553600\n",
            "2024-08-19 16:25:30,686 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-19 16:25:30,714 INFO mapred.Task: Task:attempt_local233258962_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:25:30,718 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
            "2024-08-19 16:25:30,719 INFO mapred.Task: Task 'attempt_local233258962_0001_m_000000_0' done.\n",
            "2024-08-19 16:25:30,729 INFO mapred.Task: Final Counters for attempt_local233258962_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=178138\n",
            "\t\tFILE: Number of bytes written=726676\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1769\n",
            "\t\tMap output materialized bytes=2131\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=178\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "2024-08-19 16:25:30,729 INFO mapred.LocalJobRunner: Finishing task: attempt_local233258962_0001_m_000000_0\n",
            "2024-08-19 16:25:30,730 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-19 16:25:30,734 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-19 16:25:30,734 INFO mapred.LocalJobRunner: Starting task: attempt_local233258962_0001_r_000000_0\n",
            "2024-08-19 16:25:30,744 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:25:30,746 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:25:30,747 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:25:30,751 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@42d09f46\n",
            "2024-08-19 16:25:30,754 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:25:30,793 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-19 16:25:30,796 INFO reduce.EventFetcher: attempt_local233258962_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-19 16:25:30,832 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local233258962_0001_m_000000_0 decomp: 2127 len: 2131 to MEMORY\n",
            "2024-08-19 16:25:30,837 INFO reduce.InMemoryMapOutput: Read 2127 bytes from map-output for attempt_local233258962_0001_m_000000_0\n",
            "2024-08-19 16:25:30,838 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2127, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2127\n",
            "2024-08-19 16:25:30,840 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-19 16:25:30,841 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:25:30,841 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-19 16:25:30,850 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:25:30,851 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2119 bytes\n",
            "2024-08-19 16:25:30,854 INFO reduce.MergeManagerImpl: Merged 1 segments, 2127 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-19 16:25:30,855 INFO reduce.MergeManagerImpl: Merging 1 files, 2131 bytes from disk\n",
            "2024-08-19 16:25:30,856 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-19 16:25:30,856 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:25:30,857 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2119 bytes\n",
            "2024-08-19 16:25:30,860 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:25:30,868 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer.py]\n",
            "2024-08-19 16:25:30,873 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-08-19 16:25:30,875 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-08-19 16:25:30,909 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:25:30,910 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:25:30,912 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:25:30,970 INFO streaming.PipeMapRed: Records R/W=178/1\n",
            "2024-08-19 16:25:30,977 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-19 16:25:30,978 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-19 16:25:30,979 INFO mapred.Task: Task:attempt_local233258962_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:25:30,980 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:25:30,980 INFO mapred.Task: Task attempt_local233258962_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-19 16:25:30,983 INFO output.FileOutputCommitter: Saved output of task 'attempt_local233258962_0001_r_000000_0' to file:/word_count_with_python/output\n",
            "2024-08-19 16:25:30,986 INFO mapred.LocalJobRunner: Records R/W=178/1 > reduce\n",
            "2024-08-19 16:25:30,986 INFO mapred.Task: Task 'attempt_local233258962_0001_r_000000_0' done.\n",
            "2024-08-19 16:25:30,986 INFO mapred.Task: Final Counters for attempt_local233258962_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=182432\n",
            "\t\tFILE: Number of bytes written=730106\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=2131\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=178\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:25:30,987 INFO mapred.LocalJobRunner: Finishing task: attempt_local233258962_0001_r_000000_0\n",
            "2024-08-19 16:25:30,987 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-19 16:25:31,305 INFO mapreduce.Job: Job job_local233258962_0001 running in uber mode : false\n",
            "2024-08-19 16:25:31,306 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-19 16:25:31,307 INFO mapreduce.Job: Job job_local233258962_0001 completed successfully\n",
            "2024-08-19 16:25:31,316 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=360570\n",
            "\t\tFILE: Number of bytes written=1456782\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1769\n",
            "\t\tMap output materialized bytes=2131\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=2131\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=356\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:25:31,316 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idPrkXI_V7lY",
        "outputId": "754380bc-ddfb-4d0c-ce31-dbf7e53e47bd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-19 16:25 /word_count_with_python/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1279 2024-08-19 16:25 /word_count_with_python/output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiWylcASV-U4",
        "outputId": "215be102-1a91-4662-fd7d-a2ef61d01913"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "Negara\t4\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "Republik\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from HDFS to the local file system ->\n",
        "!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /content/hdfs-wordcount.txt"
      ],
      "metadata": {
        "id": "kcKQ2MFYWBlq"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}