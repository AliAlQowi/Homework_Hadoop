{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSDPIhry5BDhGaVh401qXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAlQowi/Homework_Hadoop/blob/main/Homework_Hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeyfiFchMykb",
        "outputId": "7dbcc5ce-9309-474d-8ff1-7b1fe0c744a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n",
            "openjdk version \"1.8.0_422\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_422-8u422-b05-1~22.04-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.422-b05, mixed mode)\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n",
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "#Port 22\n",
            "#GatewayPorts no\n",
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:mngIwYSL0MkbU/kTl5sfXixbiAVy2Zl8DwKGGt36jbc root@df7268e7c342\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "| +.o.o.o=B o     |\n",
            "|oo* o ++= B o    |\n",
            "|o.o+ + + = = o   |\n",
            "|o ... + + + + .  |\n",
            "|  .    oS= *     |\n",
            "|   . o oo *      |\n",
            "|    o +  . .     |\n",
            "|     .    E      |\n",
            "|                 |\n",
            "+----[SHA256]-----+\n",
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCPb3+/ITaCqlXBsb3sxtmjYqt59plTXjytcPKnYKYYmc8Nk0wCaC569ecokL+/\n",
            "s8EVKjGezGUo6O7gPUh/HWJ/s6MlEYfTXmRXPCif7QumLgX2Z2xC4VowZXNpHZAk4KqpcpY/RMto55Pvm7lNKIDO9muaAgm1HOda\n",
            "UCx747/k9zYbklrPTdVoRkDZg5pun7VyNsTPjvRfatWCTGaTT7rn3UPtYma3Q/5L89joDYEO8xhbafDwat9Cco2ydevckx1E1xwg\n",
            "W+Caaq0ydDSQcoZ0iDaUOWnGRZ5eGJD4UGmq43IuJMXATsEY2WJtumIeMJQaXEzj2kJQoY1DsOPUTF0bz5WhrPclWMbKHY1g6TzU\n",
            "Mp0BZ5SHa8EvpEEogJmHfbFKqcrmnGl4tRQZjG6NhqoNQqr/CtQsREtHs+JDho8HF+BXSN7YJ2XKQAwl//IOK15dGXNJjXyeJXMV\n",
            "Je+fg9+90gHsHU1M0j3olBEuxBoO3v6dcOhPuStmYFmvvvhvDwE= root@df7268e7c342\n",
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
            " 16:07:11 up 6 min,  0 users,  load average: 0.79, 0.68, 0.39\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!java -version\n",
        "\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
        "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps\n",
        "!java -version\n",
        "\n",
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "!apt-get install openssh-server -qq > /dev/null\n",
        "!service ssh start\n",
        "\n",
        "!grep Port /etc/ssh/sshd_config\n",
        "\n",
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa <<< y\n",
        "\n",
        "# See id_rsa.pub content\n",
        "!more /root/.ssh/id_rsa.pub\n",
        "\n",
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime\n",
        "\n",
        "\n",
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "\n",
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively\n",
        "\n",
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh\n",
        "\n",
        "import os\n",
        "#Creating environment variables\n",
        "#Creating Hadoop home variable\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += f'{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"JRE_HOME\"]}/bin:{os.environ[\"HADOOP_HOME\"]}/sbin'\n",
        "\n",
        "#Dowloading text example to use as input\n",
        "!wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format\n",
        "\n",
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\"\n",
        "\n",
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "\n",
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh\n",
        "\n",
        "#Listing the running deamons\n",
        "!jps\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnEgJXVSUVeQ",
        "outputId": "a1acb90e-9d55-4dae-a74e-76f47bf717a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2024-08-19 16:08:16,020 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = df7268e7c342/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_422\n",
            "************************************************************/\n",
            "2024-08-19 16:08:16,101 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-08-19 16:08:16,333 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-3e4678ad-d837-4547-8070-7811d413af50\n",
            "2024-08-19 16:08:17,692 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-08-19 16:08:17,744 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-08-19 16:08:17,752 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-08-19 16:08:17,760 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-08-19 16:08:17,773 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2024-08-19 16:08:17,773 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2024-08-19 16:08:17,773 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2024-08-19 16:08:17,773 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-08-19 16:08:17,859 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-08-19 16:08:17,877 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2024-08-19 16:08:17,877 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-08-19 16:08:17,885 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-08-19 16:08:17,886 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Aug 19 16:08:17\n",
            "2024-08-19 16:08:17,888 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-08-19 16:08:17,888 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:08:17,890 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-08-19 16:08:17,890 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-08-19 16:08:17,950 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-08-19 16:08:17,950 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-08-19 16:08:17,963 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2024-08-19 16:08:17,964 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-08-19 16:08:17,964 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-08-19 16:08:17,965 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-08-19 16:08:18,043 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-08-19 16:08:18,043 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:08:18,043 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:08:18,043 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-08-19 16:08:18,105 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-08-19 16:08:18,105 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:08:18,106 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-08-19 16:08:18,106 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-08-19 16:08:18,112 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2024-08-19 16:08:18,112 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-08-19 16:08:18,112 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-08-19 16:08:18,113 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-08-19 16:08:18,125 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-08-19 16:08:18,128 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-08-19 16:08:18,136 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-08-19 16:08:18,136 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:08:18,137 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-08-19 16:08:18,137 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-08-19 16:08:18,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-08-19 16:08:18,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-08-19 16:08:18,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-08-19 16:08:18,164 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-08-19 16:08:18,164 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-08-19 16:08:18,171 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-08-19 16:08:18,171 INFO util.GSet: VM type       = 64-bit\n",
            "2024-08-19 16:08:18,171 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-08-19 16:08:18,171 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-08-19 16:08:18,254 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1427721057-172.28.0.12-1724083698235\n",
            "2024-08-19 16:08:18,296 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-08-19 16:08:18,353 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-08-19 16:08:18,665 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2024-08-19 16:08:18,703 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-08-19 16:08:18,803 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-08-19 16:08:18,803 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-08-19 16:08:18,814 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-08-19 16:08:18,815 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at df7268e7c342/172.28.0.12\n",
            "************************************************************/\n",
            "Starting namenodes on [df7268e7c342]\n",
            "df7268e7c342: Warning: Permanently added 'df7268e7c342' (ED25519) to the list of known hosts.\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [df7268e7c342]\n",
            "nohup: ignoring input and appending output to 'nohup.out'\n",
            "3122 NodeManager\n",
            "3015 ResourceManager\n",
            "3256 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQNKHrgEUbgS",
        "outputId": "3a00463c-e135-4b86-df6e-1e626c7dcd5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report: FileSystem file:/// is not an HDFS file system. The fs class is: org.apache.hadoop.fs.LocalFileSystem\n",
            "Usage: hdfs dfsadmin [-report] [-live] [-dead] [-decommissioning] [-enteringmaintenance] [-inmaintenance]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True: pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "ogQH-71cUphx",
        "outputId": "c9723c06-ec4a-4cb4-e826-2f6ca07c979f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b16dc615ea65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt\n",
        "\n",
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count\n",
        "#Coping file from local file system to HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count\n",
        "\n",
        "#Exploring Hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count\n",
        "\n",
        "# Run MapReduce Example using JAVA\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /word_count/pembukaan_uud1945.txt /word_count/output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4biLWfSjUsMI",
        "outputId": "94e24dcb-44f9-4458-8150-6f833f2037bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `/word_count': File exists\n",
            "Found 1 items\n",
            "-rw-r--r--   1 root root       1423 2024-08-19 16:09 /word_count/pembukaan_uud1945.txt\n",
            "2024-08-19 16:09:45,759 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-19 16:09:45,854 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-19 16:09:45,854 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-19 16:09:46,075 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-08-19 16:09:46,113 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-19 16:09:46,281 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local952462657_0001\n",
            "2024-08-19 16:09:46,281 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-19 16:09:46,507 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-19 16:09:46,509 INFO mapreduce.Job: Running job: job_local952462657_0001\n",
            "2024-08-19 16:09:46,514 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-19 16:09:46,525 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:09:46,525 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:09:46,527 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-08-19 16:09:46,579 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-19 16:09:46,579 INFO mapred.LocalJobRunner: Starting task: attempt_local952462657_0001_m_000000_0\n",
            "2024-08-19 16:09:46,610 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:09:46,610 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:09:46,641 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:09:46,648 INFO mapred.MapTask: Processing split: file:/word_count/pembukaan_uud1945.txt:0+1423\n",
            "2024-08-19 16:09:46,730 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-19 16:09:46,730 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-19 16:09:46,730 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-19 16:09:46,730 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-19 16:09:46,730 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-19 16:09:46,735 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-19 16:09:46,748 INFO mapred.LocalJobRunner: \n",
            "2024-08-19 16:09:46,748 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-19 16:09:46,749 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-19 16:09:46,749 INFO mapred.MapTask: bufstart = 0; bufend = 2125; bufvoid = 104857600\n",
            "2024-08-19 16:09:46,749 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213688(104854752); length = 709/6553600\n",
            "2024-08-19 16:09:46,783 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-19 16:09:46,798 INFO mapred.Task: Task:attempt_local952462657_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:09:46,802 INFO mapred.LocalJobRunner: map\n",
            "2024-08-19 16:09:46,802 INFO mapred.Task: Task 'attempt_local952462657_0001_m_000000_0' done.\n",
            "2024-08-19 16:09:46,811 INFO mapred.Task: Final Counters for attempt_local952462657_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318072\n",
            "\t\tFILE: Number of bytes written=862514\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=2125\n",
            "\t\tMap output materialized bytes=1764\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=178\n",
            "\t\tCombine output records=120\n",
            "\t\tSpilled Records=120\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "2024-08-19 16:09:46,812 INFO mapred.LocalJobRunner: Finishing task: attempt_local952462657_0001_m_000000_0\n",
            "2024-08-19 16:09:46,812 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-19 16:09:46,816 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-19 16:09:46,816 INFO mapred.LocalJobRunner: Starting task: attempt_local952462657_0001_r_000000_0\n",
            "2024-08-19 16:09:46,826 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:09:46,826 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:09:46,827 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:09:46,830 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3587ec07\n",
            "2024-08-19 16:09:46,832 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:09:46,855 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-19 16:09:46,864 INFO reduce.EventFetcher: attempt_local952462657_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-19 16:09:46,907 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local952462657_0001_m_000000_0 decomp: 1760 len: 1764 to MEMORY\n",
            "2024-08-19 16:09:46,911 INFO reduce.InMemoryMapOutput: Read 1760 bytes from map-output for attempt_local952462657_0001_m_000000_0\n",
            "2024-08-19 16:09:46,913 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1760, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1760\n",
            "2024-08-19 16:09:46,915 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-19 16:09:46,916 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:09:46,917 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-19 16:09:46,931 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:09:46,931 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1752 bytes\n",
            "2024-08-19 16:09:46,937 INFO reduce.MergeManagerImpl: Merged 1 segments, 1760 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-19 16:09:46,939 INFO reduce.MergeManagerImpl: Merging 1 files, 1764 bytes from disk\n",
            "2024-08-19 16:09:46,941 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-19 16:09:46,941 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:09:46,943 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1752 bytes\n",
            "2024-08-19 16:09:46,944 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:09:46,950 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-19 16:09:46,958 INFO mapred.Task: Task:attempt_local952462657_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:09:46,959 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:09:46,959 INFO mapred.Task: Task attempt_local952462657_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-19 16:09:46,962 INFO output.FileOutputCommitter: Saved output of task 'attempt_local952462657_0001_r_000000_0' to file:/word_count/output\n",
            "2024-08-19 16:09:46,963 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-08-19 16:09:46,963 INFO mapred.Task: Task 'attempt_local952462657_0001_r_000000_0' done.\n",
            "2024-08-19 16:09:46,965 INFO mapred.Task: Final Counters for attempt_local952462657_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=321632\n",
            "\t\tFILE: Number of bytes written=865577\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=1764\n",
            "\t\tReduce input records=120\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=120\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:09:46,965 INFO mapred.LocalJobRunner: Finishing task: attempt_local952462657_0001_r_000000_0\n",
            "2024-08-19 16:09:46,965 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-19 16:09:47,515 INFO mapreduce.Job: Job job_local952462657_0001 running in uber mode : false\n",
            "2024-08-19 16:09:47,517 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-19 16:09:47,518 INFO mapreduce.Job: Job job_local952462657_0001 completed successfully\n",
            "2024-08-19 16:09:47,535 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=639704\n",
            "\t\tFILE: Number of bytes written=1728091\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=2125\n",
            "\t\tMap output materialized bytes=1764\n",
            "\t\tInput split bytes=103\n",
            "\t\tCombine input records=178\n",
            "\t\tCombine output records=120\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=1764\n",
            "\t\tReduce input records=120\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=240\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usyn6eK_VHBQ",
        "outputId": "a3f6e473-a1c1-4bbd-cbfc-655b49bcaa3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-19 16:09 /word_count/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1279 2024-08-19 16:09 /word_count/output/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count/output/part-r-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-wMrI5eVMz4",
        "outputId": "b3049f4f-77f3-4b10-90d7-7ae5116f1a6a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "Negara\t4\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "Republik\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Exploring Hadoop utilities available\n",
        "!ls $HADOOP_HOME/share/hadoop/tools/lib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTSLzetZVTTo",
        "outputId": "0760e8be-76d5-4bb3-d597-2f90c5577a3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aliyun-java-sdk-core-4.5.10.jar      hadoop-gridmix-3.2.3.jar\n",
            "aliyun-java-sdk-kms-2.11.0.jar\t     hadoop-kafka-3.2.3.jar\n",
            "aliyun-java-sdk-ram-3.1.0.jar\t     hadoop-openstack-3.2.3.jar\n",
            "aliyun-sdk-oss-3.13.0.jar\t     hadoop-resourceestimator-3.2.3.jar\n",
            "aws-java-sdk-bundle-1.11.901.jar     hadoop-rumen-3.2.3.jar\n",
            "azure-data-lake-store-sdk-2.2.9.jar  hadoop-sls-3.2.3.jar\n",
            "azure-keyvault-core-1.0.0.jar\t     hadoop-streaming-3.2.3.jar\n",
            "azure-storage-7.0.0.jar\t\t     ini4j-0.5.4.jar\n",
            "hadoop-aliyun-3.2.3.jar\t\t     jdom2-2.0.6.jar\n",
            "hadoop-archive-logs-3.2.3.jar\t     kafka-clients-2.8.1.jar\n",
            "hadoop-archives-3.2.3.jar\t     lz4-java-1.7.1.jar\n",
            "hadoop-aws-3.2.3.jar\t\t     ojalgo-43.0.jar\n",
            "hadoop-azure-3.2.3.jar\t\t     opentracing-api-0.33.0.jar\n",
            "hadoop-azure-datalake-3.2.3.jar      opentracing-noop-0.33.0.jar\n",
            "hadoop-datajoin-3.2.3.jar\t     opentracing-util-0.33.0.jar\n",
            "hadoop-distcp-3.2.3.jar\t\t     org.jacoco.agent-0.8.5-runtime.jar\n",
            "hadoop-extras-3.2.3.jar\t\t     wildfly-openssl-1.0.7.Final.jar\n",
            "hadoop-fs2img-3.2.3.jar\t\t     zstd-jni-1.4.9-1.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input (if it has not been donwloaded yet)\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "4jayJiOFVVAH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /word_count_with_python"
      ],
      "metadata": {
        "id": "8sPzg5GDVXxQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the file from local file system to Hadoop distributed file system (HDFS)\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/pembukaan_uud1945.txt /word_count_with_python"
      ],
      "metadata": {
        "id": "WPV9VGMrVbuX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   with open('/content/pembukaan_uud1945.txt', 'r') as file:\n",
        "#         contents = file.read()\n",
        "#   print(contents)\n",
        "# except Exception as e:\n",
        "#     print(\"There is a Problem\", str(e))"
      ],
      "metadata": {
        "id": "lUP_I1HfVfYP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\n",
        "\n",
        "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\n",
        "\n",
        "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\n",
        "\n",
        "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia."
      ],
      "metadata": {
        "id": "7gWHqmwtVkh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#'#!' is known as shebang and used for interpreting the script\n",
        "\n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the line into words, output data type list\n",
        "  words = line.split()\n",
        "\n",
        "  # we are looping over the words array and printing the word\n",
        "  # with the count of 1 to the STDOUT\n",
        "  for word in words:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    # what we output here will be the input for the\n",
        "    # Reduce step, i.e. the input for reducer.py\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LVWSe6yVloI",
        "outputId": "00f1a033-e72f-4c62-931b-6ba7abbd2114"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/pembukaan_uud1945.txt', 'r') as file:\n",
        "#         line = file.read()\n",
        "line = \"\"\"\n",
        "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\n",
        "\n",
        "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\n",
        "\n",
        "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\n",
        "\n",
        "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia.\n",
        "\"\"\"\n",
        "# print(line)\n",
        "line = line.strip()\n",
        "words = line.split()\n",
        "\n",
        "for word in words:\n",
        "  data = '%s\\t%s' % (word, 1)\n",
        "\n",
        "  word, count = data.split('\\t', 1)\n",
        "  print(word, count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60dj7q0JVqKX",
        "outputId": "bc8ae4e8-ee45-4829-e04f-a4e624c029f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bahwa 1\n",
            "sesungguhnya 1\n",
            "Kemerdekaan 1\n",
            "itu 1\n",
            "ialah 1\n",
            "hak 1\n",
            "segala 1\n",
            "bangsa 1\n",
            "dan 1\n",
            "oleh 1\n",
            "sebab 1\n",
            "itu, 1\n",
            "maka 1\n",
            "penjajahan 1\n",
            "di 1\n",
            "atas 1\n",
            "dunia 1\n",
            "harus 1\n",
            "dihapuskan, 1\n",
            "karena 1\n",
            "tidak 1\n",
            "sesuai 1\n",
            "dengan 1\n",
            "perikemanusiaan 1\n",
            "dan 1\n",
            "perikeadilan. 1\n",
            "Dan 1\n",
            "perjuangan 1\n",
            "pergerakan 1\n",
            "kemerdekaan 1\n",
            "Indonesia 1\n",
            "telah 1\n",
            "sampailah 1\n",
            "kepada 1\n",
            "saat 1\n",
            "yang 1\n",
            "berbahagia 1\n",
            "dengan 1\n",
            "selamat 1\n",
            "sentausa 1\n",
            "mengantarkan 1\n",
            "rakyat 1\n",
            "Indonesia 1\n",
            "ke 1\n",
            "depan 1\n",
            "pintu 1\n",
            "gerbang 1\n",
            "kemerdekaan 1\n",
            "Negara 1\n",
            "Indonesia, 1\n",
            "yang 1\n",
            "merdeka, 1\n",
            "bersatu, 1\n",
            "berdaulat, 1\n",
            "adil 1\n",
            "dan 1\n",
            "makmur. 1\n",
            "Atas 1\n",
            "berkat 1\n",
            "rakhmat 1\n",
            "Allah 1\n",
            "Yang 1\n",
            "Maha 1\n",
            "Kuasa 1\n",
            "dan 1\n",
            "dengan 1\n",
            "didorongkan 1\n",
            "oleh 1\n",
            "keinginan 1\n",
            "luhur, 1\n",
            "supaya 1\n",
            "berkehidupan 1\n",
            "kebangsaan 1\n",
            "yang 1\n",
            "bebas, 1\n",
            "maka 1\n",
            "rakyat 1\n",
            "Indonesia 1\n",
            "menyatakan 1\n",
            "dengan 1\n",
            "ini 1\n",
            "kemerdekaannya. 1\n",
            "Kemudian 1\n",
            "daripada 1\n",
            "itu 1\n",
            "untuk 1\n",
            "membentuk 1\n",
            "suatu 1\n",
            "Pemerintah 1\n",
            "Negara 1\n",
            "Indonesia 1\n",
            "yang 1\n",
            "melindungi 1\n",
            "segenap 1\n",
            "bangsa 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "seluruh 1\n",
            "tumpah 1\n",
            "darah 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "untuk 1\n",
            "memajukan 1\n",
            "kesejahteraan 1\n",
            "umum, 1\n",
            "mencerdaskan 1\n",
            "kehidupan 1\n",
            "bangsa, 1\n",
            "dan 1\n",
            "ikut 1\n",
            "melaksanakan 1\n",
            "ketertiban 1\n",
            "dunia 1\n",
            "yang 1\n",
            "berdasarkan 1\n",
            "kemerdekaan, 1\n",
            "perdamaian 1\n",
            "abadi 1\n",
            "dan 1\n",
            "keadilan 1\n",
            "sosial, 1\n",
            "maka 1\n",
            "disusunlah 1\n",
            "Kemerdekaan 1\n",
            "Kebangsaan 1\n",
            "Indonesia 1\n",
            "itu 1\n",
            "dalam 1\n",
            "suatu 1\n",
            "Undang-Undang 1\n",
            "Dasar 1\n",
            "Negara 1\n",
            "Indonesia, 1\n",
            "yang 1\n",
            "terbentuk 1\n",
            "dalam 1\n",
            "suatu 1\n",
            "susunan 1\n",
            "Negara 1\n",
            "Republik 1\n",
            "Indonesia 1\n",
            "yang 1\n",
            "berkedaulatan 1\n",
            "rakyat 1\n",
            "dengan 1\n",
            "berdasar 1\n",
            "kepada 1\n",
            "Ketuhanan 1\n",
            "Yang 1\n",
            "Maha 1\n",
            "Esa, 1\n",
            "Kemanusiaan 1\n",
            "yang 1\n",
            "adil 1\n",
            "dan 1\n",
            "beradab, 1\n",
            "Persatuan 1\n",
            "Indonesia 1\n",
            "dan 1\n",
            "Kerakyatan 1\n",
            "yang 1\n",
            "dipimpin 1\n",
            "oleh 1\n",
            "hikmat 1\n",
            "kebijaksanaan 1\n",
            "dalam 1\n",
            "Permusyawaratan/Perwakilan, 1\n",
            "serta 1\n",
            "dengan 1\n",
            "mewujudkan 1\n",
            "suatu 1\n",
            "Keadilan 1\n",
            "sosial 1\n",
            "bagi 1\n",
            "seluruh 1\n",
            "rakyat 1\n",
            "Indonesia. 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  word, count = line.split('\\t', 1)\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_word == word:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_word: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_word, current_count))\n",
        "    current_count = count\n",
        "    current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "  print('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKpVSnMBVwOJ",
        "outputId": "811c45f9-3081-413f-fdd6-df7356aac547"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing our MapReduce job locally (Hadoop does not participate here)\n",
        "!cat pembukaan_uud1945.txt | python mapper.py | sort -k1,1 | python reducer.py | head -50\n",
        "#We apply sorting after the mapper because it is the default operation in MapReduce architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtItdkhtVxo3",
        "outputId": "aeefaab3-f72b-47b6-e5b1-fa22116a59e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abadi\t1\n",
            "adil\t2\n",
            "Allah\t1\n",
            "atas\t1\n",
            "Atas\t1\n",
            "bagi\t1\n",
            "Bahwa\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "Dan\t1\n",
            "darah\t1\n",
            "daripada\t1\n",
            "Dasar\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n",
            "dipimpin\t1\n",
            "disusunlah\t1\n",
            "dunia\t2\n",
            "Esa,\t1\n",
            "gerbang\t1\n",
            "hak\t1\n",
            "harus\t1\n",
            "hikmat\t1\n",
            "ialah\t1\n",
            "ikut\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "ini\t1\n",
            "itu\t3\n",
            "itu,\t1\n",
            "karena\t1\n",
            "ke\t1\n",
            "keadilan\t1\n",
            "Keadilan\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the files\n",
        "!chmod 777 /content/mapper.py /content/reducer.py\n",
        "#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users"
      ],
      "metadata": {
        "id": "h9VGxYObV1vv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running MapReduce programs\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /word_count_with_python/pembukaan_uud1945.txt \\\n",
        "  -output /word_count_with_python/output \\\n",
        "  -mapper \"python /content/mapper.py\" \\\n",
        "  -reducer \"python /content/reducer.py\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpBYMQL8V3i4",
        "outputId": "211a0654-d9cb-45f5-813e-3f0c4c3e024e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-19 16:10:31,237 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-08-19 16:10:31,352 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-08-19 16:10:31,352 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-08-19 16:10:31,373 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:10:31,615 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-08-19 16:10:31,646 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-08-19 16:10:31,951 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local351546029_0001\n",
            "2024-08-19 16:10:31,951 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-08-19 16:10:32,220 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-08-19 16:10:32,222 INFO mapreduce.Job: Running job: job_local351546029_0001\n",
            "2024-08-19 16:10:32,228 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-08-19 16:10:32,236 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-08-19 16:10:32,245 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:10:32,245 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:10:32,297 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-08-19 16:10:32,302 INFO mapred.LocalJobRunner: Starting task: attempt_local351546029_0001_m_000000_0\n",
            "2024-08-19 16:10:32,337 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:10:32,337 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:10:32,375 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:10:32,391 INFO mapred.MapTask: Processing split: file:/word_count_with_python/pembukaan_uud1945.txt:0+1423\n",
            "2024-08-19 16:10:32,406 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-08-19 16:10:32,592 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-08-19 16:10:32,592 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-08-19 16:10:32,592 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-08-19 16:10:32,592 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-08-19 16:10:32,592 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-08-19 16:10:32,596 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-08-19 16:10:32,603 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/mapper.py]\n",
            "2024-08-19 16:10:32,609 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-08-19 16:10:32,610 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-08-19 16:10:32,611 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-08-19 16:10:32,611 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-08-19 16:10:32,612 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-08-19 16:10:32,612 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-08-19 16:10:32,612 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-08-19 16:10:32,616 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-08-19 16:10:32,616 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-08-19 16:10:32,619 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-08-19 16:10:32,619 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-08-19 16:10:32,620 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-08-19 16:10:32,652 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:10:32,716 INFO streaming.PipeMapRed: Records R/W=7/1\n",
            "2024-08-19 16:10:32,723 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-19 16:10:32,724 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-19 16:10:32,728 INFO mapred.LocalJobRunner: \n",
            "2024-08-19 16:10:32,728 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-08-19 16:10:32,728 INFO mapred.MapTask: Spilling map output\n",
            "2024-08-19 16:10:32,728 INFO mapred.MapTask: bufstart = 0; bufend = 1769; bufvoid = 104857600\n",
            "2024-08-19 16:10:32,728 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213688(104854752); length = 709/6553600\n",
            "2024-08-19 16:10:32,745 INFO mapred.MapTask: Finished spill 0\n",
            "2024-08-19 16:10:32,760 INFO mapred.Task: Task:attempt_local351546029_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:10:32,763 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
            "2024-08-19 16:10:32,763 INFO mapred.Task: Task 'attempt_local351546029_0001_m_000000_0' done.\n",
            "2024-08-19 16:10:32,772 INFO mapred.Task: Final Counters for attempt_local351546029_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=178138\n",
            "\t\tFILE: Number of bytes written=726676\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1769\n",
            "\t\tMap output materialized bytes=2131\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=178\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "2024-08-19 16:10:32,772 INFO mapred.LocalJobRunner: Finishing task: attempt_local351546029_0001_m_000000_0\n",
            "2024-08-19 16:10:32,773 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-08-19 16:10:32,781 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-08-19 16:10:32,782 INFO mapred.LocalJobRunner: Starting task: attempt_local351546029_0001_r_000000_0\n",
            "2024-08-19 16:10:32,792 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-08-19 16:10:32,792 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-08-19 16:10:32,793 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-08-19 16:10:32,797 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5aaa23f4\n",
            "2024-08-19 16:10:32,799 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-08-19 16:10:32,839 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-08-19 16:10:32,850 INFO reduce.EventFetcher: attempt_local351546029_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-08-19 16:10:32,894 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local351546029_0001_m_000000_0 decomp: 2127 len: 2131 to MEMORY\n",
            "2024-08-19 16:10:32,899 INFO reduce.InMemoryMapOutput: Read 2127 bytes from map-output for attempt_local351546029_0001_m_000000_0\n",
            "2024-08-19 16:10:32,901 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2127, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2127\n",
            "2024-08-19 16:10:32,904 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-08-19 16:10:32,905 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:10:32,905 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-08-19 16:10:32,921 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:10:32,921 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2119 bytes\n",
            "2024-08-19 16:10:32,928 INFO reduce.MergeManagerImpl: Merged 1 segments, 2127 bytes to disk to satisfy reduce memory limit\n",
            "2024-08-19 16:10:32,929 INFO reduce.MergeManagerImpl: Merging 1 files, 2131 bytes from disk\n",
            "2024-08-19 16:10:32,931 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-08-19 16:10:32,931 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-08-19 16:10:32,933 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2119 bytes\n",
            "2024-08-19 16:10:32,934 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:10:32,943 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, /content/reducer.py]\n",
            "2024-08-19 16:10:32,947 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-08-19 16:10:32,948 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-08-19 16:10:32,988 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:10:32,989 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:10:32,990 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-08-19 16:10:33,046 INFO streaming.PipeMapRed: Records R/W=178/1\n",
            "2024-08-19 16:10:33,070 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-08-19 16:10:33,074 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-08-19 16:10:33,075 INFO mapred.Task: Task:attempt_local351546029_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-08-19 16:10:33,078 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-08-19 16:10:33,079 INFO mapred.Task: Task attempt_local351546029_0001_r_000000_0 is allowed to commit now\n",
            "2024-08-19 16:10:33,085 INFO output.FileOutputCommitter: Saved output of task 'attempt_local351546029_0001_r_000000_0' to file:/word_count_with_python/output\n",
            "2024-08-19 16:10:33,088 INFO mapred.LocalJobRunner: Records R/W=178/1 > reduce\n",
            "2024-08-19 16:10:33,088 INFO mapred.Task: Task 'attempt_local351546029_0001_r_000000_0' done.\n",
            "2024-08-19 16:10:33,090 INFO mapred.Task: Final Counters for attempt_local351546029_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=182432\n",
            "\t\tFILE: Number of bytes written=730106\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=2131\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=178\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:10:33,091 INFO mapred.LocalJobRunner: Finishing task: attempt_local351546029_0001_r_000000_0\n",
            "2024-08-19 16:10:33,091 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-08-19 16:10:33,235 INFO mapreduce.Job: Job job_local351546029_0001 running in uber mode : false\n",
            "2024-08-19 16:10:33,236 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-08-19 16:10:33,238 INFO mapreduce.Job: Job job_local351546029_0001 completed successfully\n",
            "2024-08-19 16:10:33,249 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=360570\n",
            "\t\tFILE: Number of bytes written=1456782\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=7\n",
            "\t\tMap output records=178\n",
            "\t\tMap output bytes=1769\n",
            "\t\tMap output materialized bytes=2131\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=120\n",
            "\t\tReduce shuffle bytes=2131\n",
            "\t\tReduce input records=178\n",
            "\t\tReduce output records=120\n",
            "\t\tSpilled Records=356\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1447\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1299\n",
            "2024-08-19 16:10:33,250 INFO streaming.StreamJob: Output directory: /word_count_with_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the created output directory\n",
        "#part-r-00000 contains the actual ouput\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /word_count_with_python/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idPrkXI_V7lY",
        "outputId": "6d4c7f4e-000f-484a-ebde-ded3e159fbef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-08-19 16:10 /word_count_with_python/output/_SUCCESS\n",
            "-rw-r--r--   1 root root       1279 2024-08-19 16:10 /word_count_with_python/output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out first 50 lines\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /word_count_with_python/output/part-00000 | head -50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiWylcASV-U4",
        "outputId": "e66f8bac-63d6-40f5-d1dd-8a952429a4dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "Negara\t4\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "Republik\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy a file from HDFS to the local file system ->\n",
        "!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /word_count_with_python/output/part-00000 /content/hdfs-wordcount.txt"
      ],
      "metadata": {
        "id": "kcKQ2MFYWBlq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote origin https://<PAT>github.com/AliAlQowi/Homework_Hadoop.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZTsd-eBZ8kw",
        "outputId": "c454a06a-b9c1-497a-fec4-92b0ee1859a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: PAT: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}